import { GoogleGenAI, GenerateContentResponse, Chat, Modality, Type, LiveSession, GenerateVideosOperation } from "@google/genai";
import type { Message, GroundingSource } from "../types";

const API_KEY = process.env.API_KEY;

if (!API_KEY) {
  console.warn("API_KEY environment variable not set. App will not function correctly.");
}

const getAIClient = () => new GoogleGenAI({ apiKey: API_KEY });

// Text Generation
export const generateText = async (prompt: string, imagePart: {inlineData: {data: string, mimeType: string}} | null, model: 'gemini-2.5-pro' | 'gemini-2.5-flash' | 'gemini-2.5-flash-lite'): Promise<GenerateContentResponse> => {
  const ai = getAIClient();
  const contents = imagePart ? { parts: [ {text: prompt}, imagePart ] } : prompt;
  return ai.models.generateContent({ model, contents });
};

export const generateTextWithThinking = async (prompt: string): Promise<GenerateContentResponse> => {
    const ai = getAIClient();
    return ai.models.generateContent({
        model: 'gemini-2.5-pro',
        contents: prompt,
        config: {
            thinkingConfig: { thinkingBudget: 32768 }
        }
    });
};

// Search Grounding
export const generateTextWithGrounding = async (prompt: string, tool: 'googleSearch' | 'googleMaps', location?: {latitude: number, longitude: number}): Promise<{text: string, sources: GroundingSource[]}> => {
    const ai = getAIClient();
    const config: any = { tools: [{[tool]: {}}] };
    if (tool === 'googleMaps' && location) {
        config.toolConfig = {
            retrievalConfig: { latLng: location }
        }
    }

    const response = await ai.models.generateContent({
        model: 'gemini-2.5-flash',
        contents: prompt,
        config,
    });
    
    const text = response.text;
    const rawChunks = response.candidates?.[0]?.groundingMetadata?.groundingChunks || [];
    const sources: GroundingSource[] = rawChunks.map((chunk: any) => ({
        uri: chunk.web?.uri || chunk.maps?.uri || '#',
        title: chunk.web?.title || chunk.maps?.title || 'Unknown Source'
    })).filter(s => s.uri !== '#');

    return { text, sources };
}


// Chat
export const createChat = (model: 'gemini-2.5-flash'): Chat => {
    const ai = getAIClient();
    return ai.chats.create({ model });
}

// Image Generation
export const generateImage = async (prompt: string, aspectRatio: '1:1' | '16:9' | '9:16' | '4:3' | '3:4'): Promise<string[]> => {
    const ai = getAIClient();
    const response = await ai.models.generateImages({
        model: 'imagen-4.0-generate-001',
        prompt,
        config: {
            numberOfImages: 1,
            outputMimeType: 'image/jpeg',
            aspectRatio,
        }
    });
    return response.generatedImages.map(img => img.image.imageBytes);
}

// Image Editing
export const editImage = async (prompt: string, imageData: string, mimeType: string): Promise<string> => {
    const ai = getAIClient();
    const response = await ai.models.generateContent({
        model: 'gemini-2.5-flash-image',
        contents: {
            parts: [
                { inlineData: { data: imageData, mimeType } },
                { text: prompt },
            ]
        },
        config: {
            responseModalities: [Modality.IMAGE],
        }
    });

    const part = response.candidates?.[0]?.content?.parts?.[0];
    if (part?.inlineData) {
        return part.inlineData.data;
    }
    throw new Error("No image was generated by the model.");
}

// Live Conversation
export const connectLive = async (callbacks: any): Promise<LiveSession> => {
    const ai = getAIClient();
    return ai.live.connect({
        model: 'gemini-2.5-flash-native-audio-preview-09-2025',
        callbacks,
        config: {
            responseModalities: [Modality.AUDIO],
            speechConfig: {
                voiceConfig: { prebuiltVoiceConfig: { voiceName: 'Zephyr' } },
            },
            inputAudioTranscription: {},
            outputAudioTranscription: {},
            systemInstruction: 'You are Jux, a friendly and helpful conversational AI.',
        },
    });
};

// TTS
export const generateSpeech = async (text: string): Promise<string> => {
    const ai = getAIClient();
    const response = await ai.models.generateContent({
        model: "gemini-2.5-flash-preview-tts",
        contents: [{ parts: [{ text: `Say this calmly: ${text}` }] }],
        config: {
            responseModalities: [Modality.AUDIO],
            speechConfig: {
                voiceConfig: {
                    prebuiltVoiceConfig: { voiceName: 'Kore' },
                },
            },
        },
    });
    const base64Audio = response.candidates?.[0]?.content?.parts?.[0]?.inlineData?.data;
    if (!base64Audio) throw new Error("TTS generation failed");
    return base64Audio;
}

// Fix: Add functions for video generation.
// Video Generation
export const generateVideo = async (prompt: string, image: {data: string, mimeType: string} | null, aspectRatio: '16:9' | '9:16'): Promise<GenerateVideosOperation> => {
    const ai = getAIClient();
    const operation = await ai.models.generateVideos({
        model: 'veo-3.1-fast-generate-preview',
        prompt: prompt,
        image: image ? {
            imageBytes: image.data,
            mimeType: image.mimeType,
        } : undefined,
        config: {
            numberOfVideos: 1,
            resolution: '720p',
            aspectRatio: aspectRatio,
        }
    });
    return operation;
};

export const pollVideoOperation = async (operation: GenerateVideosOperation): Promise<GenerateVideosOperation> => {
    const ai = getAIClient();
    return await ai.operations.getVideosOperation({operation: operation});
};
